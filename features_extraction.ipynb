{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sklearn.feature_extraction module** can be used to extract features from text or images in a format supported by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many machine learning problems have categorical features.\n",
    "\n",
    "- These categorical variables are commonly encoded using one-hot encoding, in which the\n",
    "  explanatory variable(or features) is encoded using one binary feature for each of the \n",
    "  variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Dummy_Variables1.PNG\"/>&nbsp;&nbsp;<img src=\"images/onek.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many machine learning problems use text as an explanatory variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ex2.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the most common representation of text that is used in machine learning:\n",
    "    \n",
    " **The bag-of-words model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of text in a format of a matrix where each row is an observation and\n",
    "  each column is a unique word. \n",
    "  \n",
    "\n",
    "- The value of each element of in a matrix is either a binary value that indicate the presence \n",
    "  of each word or an integer that indicate how many times that word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words is a representation of text that describes the occurrence of words with in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "provides a simple way that  can produce a bag-of-words representation from a collection of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A collection of documents is called a corpus.\n",
    "\n",
    "\n",
    "- Tokenization is the process of splitting  documents or a string  into tokens(words).\n",
    "\n",
    "\n",
    "- The CountVectorizer class tokenizes using a regular expression that splits strings on \n",
    "  whitespace and extracts sequences of characters that are two or more in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hey hey hey lets go get lunch today..!\",\n",
    "            \"Did you go home?..\",\n",
    "            \"Hey!!! I need a favor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using fit method ,CountVectorizer() will learn what tokens are being used in our messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using the get_feature_name() method ,we can see what features\n",
    "  have been created from our messages.(or what tokens have been learned by CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'did', u'favor', u'get', u'go', u'hey', u'home', u'lets', u'lunch', u'need', u'today', u'you']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did favor get go hey home lets lunch need today you\n"
     ]
    }
   ],
   "source": [
    "for i in vect.get_feature_names():\n",
    "    print (i,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there are few things to consider\n",
    "\n",
    " 1:Everything in lowercase\n",
    " \n",
    " 2:Words less than two letters have not been included(ex..'a')\n",
    " \n",
    " 3:Punctuation has been removed\n",
    " \n",
    " 4:there are no duplicates\n",
    " \n",
    " 5:alphabatic order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform() will create matrix to represent our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(vect.get_stop_words())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vect.transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 3 0 1 1 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 0 0 1]\n",
      " [0 1 0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#print dtm\n",
    "print (dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   did  favor  get  go  hey  home  lets  lunch  need  today  you\n",
      "0    0      0    1   1    3     0     1      1     0      1    0\n",
      "1    1      0    0   1    0     1     0      0     0      0    1\n",
      "2    0      1    0   0    1     0     0      0     1      0    0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dtm.toarray(),columns = vect.get_feature_names())\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'hello john..! how are you.@#.?'\n",
    "#hello john how are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[]\n",
    "for i in test:\n",
    "    if i not in string.punctuation:\n",
    "        d.append(i)\n",
    "        \n",
    "print (''.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg = ['Hey lets go get a drink tonight']\n",
    "new_dtm = vect.transform(new_msg)\n",
    "print new_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Term Frequency, Inverse Document Frequency\n",
    "\n",
    "- It's a way to score the importance of words (or \"terms\") in a document based on how\n",
    "  frequently they appear across multiple documents.\n",
    "  \n",
    "\n",
    "- If a word appears frequently in a document, it's important. that word will get high score.\n",
    "\n",
    "\n",
    "- But if a word appears in many documents, it's not a unique identifier. that word will get low \n",
    "  score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf score of a word(w):\n",
    "    \n",
    "       tf(w) * idf(w)\n",
    "        \n",
    "        \n",
    "- where \n",
    "  tf(w) = (Number of times the word appears in a document)/(Total no. of words in the document) \n",
    "  \n",
    "  \n",
    "- idf(w) = log(No. of documents/No. of documents that contain word w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hey  lets go get lunch today..!\",\n",
    "            \"Did you go home?..\",\n",
    "            \"Hey!!! hey heyI favor need a favor\",\n",
    "            \"I want a favor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "dtm = vect.fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dtm.toarray(),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>favor</th>\n",
       "      <th>get</th>\n",
       "      <th>go</th>\n",
       "      <th>hey</th>\n",
       "      <th>heyi</th>\n",
       "      <th>home</th>\n",
       "      <th>lets</th>\n",
       "      <th>lunch</th>\n",
       "      <th>need</th>\n",
       "      <th>today</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.344315</td>\n",
       "      <td>0.344315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597147</td>\n",
       "      <td>0.378703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        did     favor       get        go       hey      heyi      home  \\\n",
       "0  0.000000  0.000000  0.436719  0.344315  0.344315  0.000000  0.000000   \n",
       "1  0.525473  0.000000  0.000000  0.414289  0.000000  0.000000  0.525473   \n",
       "2  0.000000  0.597147  0.000000  0.000000  0.597147  0.378703  0.000000   \n",
       "3  0.000000  0.619130  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       lets     lunch      need     today      want       you  \n",
       "0  0.436719  0.436719  0.000000  0.436719  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.525473  \n",
       "2  0.000000  0.000000  0.378703  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.785288  0.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stop word filtering\n",
    "\n",
    "A strategy is to remove words that are common to most of the documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   did  favor  hey  home  lets  lunch  need  today\n",
      "0    0      0    3     0     1      1     0      1\n",
      "1    1      0    0     1     0      0     0      0\n",
      "2    0      1    1     0     0      0     1      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "messages = [\"Hey hey hey lets go get lunch today..!\",\n",
    "            \"Did you go home?..\",\n",
    "            \"Hey!!! I need a favor\"]\n",
    "\n",
    "dtm=vect.fit_transform(messages)\n",
    "\n",
    "df = pd.DataFrame(dtm.toarray(),columns = vect.get_feature_names())\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'six',\n",
       " 'less',\n",
       " 'being',\n",
       " 'indeed',\n",
       " 'over',\n",
       " 'move',\n",
       " 'anyway',\n",
       " 'fifty',\n",
       " 'four',\n",
       " 'not',\n",
       " 'own',\n",
       " 'through',\n",
       " 'yourselves',\n",
       " 'go',\n",
       " 'where',\n",
       " 'mill',\n",
       " 'only',\n",
       " 'find',\n",
       " 'before',\n",
       " 'one',\n",
       " 'whose',\n",
       " 'system',\n",
       " 'how',\n",
       " 'somewhere',\n",
       " 'with',\n",
       " 'thick',\n",
       " 'show',\n",
       " 'had',\n",
       " 'enough',\n",
       " 'should',\n",
       " 'to',\n",
       " 'must',\n",
       " 'whom',\n",
       " 'seeming',\n",
       " 'under',\n",
       " 'ours',\n",
       " 'has',\n",
       " 'might',\n",
       " 'thereafter',\n",
       " 'latterly',\n",
       " 'do',\n",
       " 'them',\n",
       " 'his',\n",
       " 'around',\n",
       " 'than',\n",
       " 'get',\n",
       " 'very',\n",
       " 'de',\n",
       " 'none',\n",
       " 'cannot',\n",
       " 'every',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'front',\n",
       " 'during',\n",
       " 'thus',\n",
       " 'now',\n",
       " 'him',\n",
       " 'nor',\n",
       " 'name',\n",
       " 'several',\n",
       " 'hereafter',\n",
       " 'always',\n",
       " 'who',\n",
       " 'cry',\n",
       " 'whither',\n",
       " 'this',\n",
       " 'someone',\n",
       " 'either',\n",
       " 'each',\n",
       " 'become',\n",
       " 'thereupon',\n",
       " 'sometime',\n",
       " 'side',\n",
       " 'two',\n",
       " 'therein',\n",
       " 'twelve',\n",
       " 'because',\n",
       " 'often',\n",
       " 'ten',\n",
       " 'our',\n",
       " 'eg',\n",
       " 'some',\n",
       " 'back',\n",
       " 'up',\n",
       " 'namely',\n",
       " 'towards',\n",
       " 'are',\n",
       " 'further',\n",
       " 'beyond',\n",
       " 'ourselves',\n",
       " 'yet',\n",
       " 'out',\n",
       " 'even',\n",
       " 'will',\n",
       " 'what',\n",
       " 'still',\n",
       " 'for',\n",
       " 'bottom',\n",
       " 'mine',\n",
       " 'since',\n",
       " 'please',\n",
       " 'forty',\n",
       " 'per',\n",
       " 'its',\n",
       " 'everything',\n",
       " 'behind',\n",
       " 'un',\n",
       " 'above',\n",
       " 'between',\n",
       " 'it',\n",
       " 'neither',\n",
       " 'seemed',\n",
       " 'ever',\n",
       " 'across',\n",
       " 'she',\n",
       " 'somehow',\n",
       " 'be',\n",
       " 'we',\n",
       " 'full',\n",
       " 'never',\n",
       " 'sixty',\n",
       " 'however',\n",
       " 'here',\n",
       " 'otherwise',\n",
       " 'were',\n",
       " 'whereupon',\n",
       " 'nowhere',\n",
       " 'although',\n",
       " 'found',\n",
       " 'alone',\n",
       " 're',\n",
       " 'along',\n",
       " 'fifteen',\n",
       " 'by',\n",
       " 'both',\n",
       " 'about',\n",
       " 'last',\n",
       " 'would',\n",
       " 'anything',\n",
       " 'via',\n",
       " 'many',\n",
       " 'could',\n",
       " 'thence',\n",
       " 'put',\n",
       " 'against',\n",
       " 'keep',\n",
       " 'etc',\n",
       " 'amount',\n",
       " 'became',\n",
       " 'ltd',\n",
       " 'hence',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'con',\n",
       " 'among',\n",
       " 'already',\n",
       " 'co',\n",
       " 'afterwards',\n",
       " 'formerly',\n",
       " 'within',\n",
       " 'seems',\n",
       " 'into',\n",
       " 'others',\n",
       " 'while',\n",
       " 'whatever',\n",
       " 'except',\n",
       " 'down',\n",
       " 'hers',\n",
       " 'everyone',\n",
       " 'done',\n",
       " 'least',\n",
       " 'another',\n",
       " 'whoever',\n",
       " 'moreover',\n",
       " 'couldnt',\n",
       " 'throughout',\n",
       " 'anyhow',\n",
       " 'yourself',\n",
       " 'three',\n",
       " 'from',\n",
       " 'her',\n",
       " 'few',\n",
       " 'together',\n",
       " 'top',\n",
       " 'there',\n",
       " 'due',\n",
       " 'been',\n",
       " 'next',\n",
       " 'anyone',\n",
       " 'eleven',\n",
       " 'much',\n",
       " 'call',\n",
       " 'therefore',\n",
       " 'interest',\n",
       " 'then',\n",
       " 'thru',\n",
       " 'themselves',\n",
       " 'hundred',\n",
       " 'was',\n",
       " 'sincere',\n",
       " 'empty',\n",
       " 'more',\n",
       " 'himself',\n",
       " 'elsewhere',\n",
       " 'mostly',\n",
       " 'on',\n",
       " 'fire',\n",
       " 'am',\n",
       " 'becoming',\n",
       " 'hereby',\n",
       " 'amongst',\n",
       " 'else',\n",
       " 'part',\n",
       " 'everywhere',\n",
       " 'too',\n",
       " 'herself',\n",
       " 'former',\n",
       " 'those',\n",
       " 'he',\n",
       " 'me',\n",
       " 'myself',\n",
       " 'made',\n",
       " 'twenty',\n",
       " 'these',\n",
       " 'bill',\n",
       " 'cant',\n",
       " 'us',\n",
       " 'until',\n",
       " 'besides',\n",
       " 'nevertheless',\n",
       " 'below',\n",
       " 'anywhere',\n",
       " 'nine',\n",
       " 'can',\n",
       " 'of',\n",
       " 'toward',\n",
       " 'my',\n",
       " 'something',\n",
       " 'and',\n",
       " 'whereafter',\n",
       " 'whenever',\n",
       " 'give',\n",
       " 'almost',\n",
       " 'wherever',\n",
       " 'is',\n",
       " 'describe',\n",
       " 'beforehand',\n",
       " 'herein',\n",
       " 'an',\n",
       " 'as',\n",
       " 'itself',\n",
       " 'at',\n",
       " 'have',\n",
       " 'in',\n",
       " 'seem',\n",
       " 'whence',\n",
       " 'ie',\n",
       " 'any',\n",
       " 'fill',\n",
       " 'again',\n",
       " 'hasnt',\n",
       " 'inc',\n",
       " 'thereby',\n",
       " 'thin',\n",
       " 'no',\n",
       " 'perhaps',\n",
       " 'latter',\n",
       " 'meanwhile',\n",
       " 'when',\n",
       " 'detail',\n",
       " 'same',\n",
       " 'wherein',\n",
       " 'beside',\n",
       " 'also',\n",
       " 'that',\n",
       " 'other',\n",
       " 'take',\n",
       " 'which',\n",
       " 'becomes',\n",
       " 'you',\n",
       " 'if',\n",
       " 'nobody',\n",
       " 'see',\n",
       " 'though',\n",
       " 'may',\n",
       " 'after',\n",
       " 'upon',\n",
       " 'most',\n",
       " 'hereupon',\n",
       " 'eight',\n",
       " 'but',\n",
       " 'serious',\n",
       " 'nothing',\n",
       " 'such',\n",
       " 'your',\n",
       " 'why',\n",
       " 'a',\n",
       " 'off',\n",
       " 'whereby',\n",
       " 'third',\n",
       " 'i',\n",
       " 'whole',\n",
       " 'noone',\n",
       " 'sometimes',\n",
       " 'well',\n",
       " 'amoungst',\n",
       " 'yours',\n",
       " 'their',\n",
       " 'rather',\n",
       " 'without',\n",
       " 'so',\n",
       " 'five',\n",
       " 'the',\n",
       " 'first',\n",
       " 'whereas',\n",
       " 'once']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
